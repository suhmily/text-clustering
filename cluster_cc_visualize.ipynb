{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725006d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# HDFS上Parquet文件的路径\n",
    "hdfs_path = \"viewfs://hadoop-lt-cluster/home/mmu_llm/dw/mmu_llm.db/customjtmath_2013_20/type=normal/part-04999-626445f5-ee23-4a80-b0bd-e35648f16988.c000.snappy.parquet\"\n",
    "\n",
    "def parse_json_content(json_str: str) -> str:\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        return json_data.get('content', '')\n",
    "    except json.JSONDecodeError:\n",
    "        return ''\n",
    "\n",
    "def read_and_parse_parquet(file_path: str, max_chunks: int = None, max_length: int = 50) -> Generator[Dict[str, str], None, None]:\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        print(f\"文件包含 {parquet_file.num_row_groups} 个行组\")\n",
    "        print(f\"文件模式: {parquet_file.schema}\")\n",
    "        \n",
    "        for i in tqdm(range(min(max_chunks or float('inf'), parquet_file.num_row_groups))):\n",
    "            table = parquet_file.read_row_group(i)\n",
    "            df = table.to_pandas()\n",
    "            if 'text' in df.columns:\n",
    "                for text in df['text']:\n",
    "                    content = parse_json_content(text)[:max_length]\n",
    "                    if content:\n",
    "                        yield content\n",
    "            print(f\"处理完第 {i+1} 个行组\")\n",
    "            if max_chunks and i + 1 >= max_chunks:\n",
    "                print(f\"已达到指定的最大块数 {max_chunks}，停止读取\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "\n",
    "# 读取并解析数据\n",
    "parsed_data = list(read_and_parse_parquet(hdfs_path, max_chunks=2))\n",
    "\n",
    "if parsed_data:\n",
    "    print(f\"\\n成功读取并解析数据\")\n",
    "    print(f\"总共解析的数据条数: {len(parsed_data)}\")\n",
    "    print(\"前5条解析后的内容:\")\n",
    "    for item in parsed_data[:5]:\n",
    "        print(item[:100] + '...')  # 只打印每条内容的前100个字符\n",
    "\n",
    "    # 将解析后的数据转换为Hugging Face Dataset格式\n",
    "    # dataset = Dataset.from_list(parsed_data)\n",
    "    texts = parsed_data\n",
    "    gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "else:\n",
    "    print(\"无法读取或解析数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.text_clustering import ClusterClassifier\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have a pandas DataFrame named pandas_df\n",
    "# Example:\n",
    "# pandas_df = pd.DataFrame({'content': [\"text1\", \"text2\", \"text3\"]})\n",
    "\n",
    "# Create an instance of ClusterClassifier\n",
    "cc = ClusterClassifier(\n",
    "    embed_device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Run the pipeline on the 'content' column\n",
    "\n",
    "embs, labels, summaries = cc.fit(texts)\n",
    "\n",
    "# Customize color scheme (optional)\n",
    "# default_cycler = (cycler(color=[\n",
    "#     \"#0F0A0A\", \"#FF6600\", \"#FFBE00\", \"#496767\", \"#87A19E\",\n",
    "#     \"#FF9200\", \"#0F3538\", \"#F8E08E\", \"#0F2021\", \"#FAFAF0\"\n",
    "# ]))\n",
    "# plt.rc('axes', prop_cycle=default_cycler)\n",
    "\n",
    "# Visualize the results\n",
    "cc.show(interactive=False)\n",
    "\n",
    "# Save the classifier (optional)\n",
    "cc.save(\"./content_clusters\")\n",
    "\n",
    "\n",
    "# If you want to classify new texts later:\n",
    "# new_texts = [\"Some new text\", \"Another new text\"]\n",
    "# cluster_labels, embeddings = cc.infer(new_texts, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b046dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmu_nlp_hdd/suzhou03/data/text-clustering/myenv_python3.8_text_clustering/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mmu_nlp_hdd/suzhou03/data/text-clustering/myenv_python3.8_text_clustering/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.text_clustering import ClusterClassifier\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have a pandas DataFrame named pandas_df\n",
    "# Example:\n",
    "# pandas_df = pd.DataFrame({'content': [\"text1\", \"text2\", \"text3\"]})\n",
    "\n",
    "# Create an instance of ClusterClassifier\n",
    "cc = ClusterClassifier(\n",
    "    embed_device=\"cuda\",\n",
    ")\n",
    "cc.load(\"./content_clusters\")\n",
    "# cc.optimize_parameters()\n",
    "\n",
    "cc.show(interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f83fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.show(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_cluster_proportions(labels, summaries):\n",
    "    # Count the number of items in each cluster\n",
    "    cluster_counts = Counter(labels)\n",
    "    \n",
    "    # Sort the clusters by their labels, excluding -1 (which is typically used for noise)\n",
    "    sorted_clusters = sorted([item for item in cluster_counts.items() if item[0] != -1])\n",
    "    \n",
    "    # Separate the labels and counts\n",
    "    cluster_labels, counts = zip(*sorted_clusters)\n",
    "    \n",
    "    # Calculate the proportion of noise points (label -1)\n",
    "    noise_count = cluster_counts.get(-1, 0)\n",
    "    total_count = sum(counts) + noise_count\n",
    "    noise_proportion = noise_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = [count / sum(counts) * 100 for count in counts]\n",
    "    \n",
    "    # Create a color palette\n",
    "    colors = sns.color_palette(\"husl\", len(cluster_labels))\n",
    "    \n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    \n",
    "    # Create a pie chart on the left subplot\n",
    "    wedges, texts, autotexts = ax1.pie(counts, autopct='%1.1f%%', startangle=90, colors=colors,\n",
    "                                       textprops=dict(color=\"w\", weight=\"bold\", size=10))\n",
    "    \n",
    "    # Add title to the pie chart\n",
    "    ax1.set_title('Proportion of Data Points in Each Cluster', fontsize=16)\n",
    "    \n",
    "    # Add a note about noise proportion\n",
    "    ax1.annotate(f'Noise: {noise_proportion:.1%}', xy=(0.95, 0.05), xycoords='axes fraction',\n",
    "                 horizontalalignment='right', verticalalignment='bottom', fontsize=10)\n",
    "    \n",
    "    # Ensure the pie chart is circular\n",
    "    ax1.axis('equal')\n",
    "    \n",
    "    # Create colored text with percentages for each cluster on the right subplot\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.axis('off')  # Turn off axes for the text subplot\n",
    "    \n",
    "    for i, (label, summary, color, percentage) in enumerate(zip(cluster_labels, summaries.values(), colors, percentages)):\n",
    "        y_position = 1 - (i + 1) / (len(cluster_labels) + 1)\n",
    "        rect = Rectangle((0, y_position), 0.03, 0.03, facecolor=color, edgecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(0.05, y_position, f'Cluster {label} ({percentage:.1f}%): {summary}', \n",
    "                 va='center', ha='left', wrap=True, fontsize=12)\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # labels = cc.cluster_labels\n",
    "# summaries = cc.cluster_summaries\n",
    "\n",
    "labels = cc.cluster_labels\n",
    "summaries = cc.cluster_summaries\n",
    "visualize_cluster_proportions(labels, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "def optimize_parameters(cc, eps_range=(0.1, 2.0), min_samples_range=(2, 20), n_trials=20):\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        eps = np.random.uniform(*eps_range)\n",
    "        min_samples = np.random.randint(*min_samples_range)\n",
    "        \n",
    "        cc.dbscan_eps = eps\n",
    "        cc.dbscan_min_samples = min_samples\n",
    "        cc.cluster_labels = cc.cluster(cc.projections)\n",
    "        \n",
    "        # 跳过全是噪声的情况\n",
    "        if len(set(cc.cluster_labels)) == 1:\n",
    "            continue\n",
    "        \n",
    "        score = silhouette_score(cc.projections, cc.cluster_labels)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = (eps, min_samples)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "best_eps, best_min_samples = optimize_parameters(cc)\n",
    "print(f\"Best parameters: eps={best_eps:.2f}, min_samples={best_min_samples}\")\n",
    "\n",
    "cc.dbscan_eps = best_eps\n",
    "cc.dbscan_min_samples = best_min_samples\n",
    "cc.cluster_labels = cc.cluster(cc.projections)\n",
    "cc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be978d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccb16d",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "noise_ratio = cc.calculate_noise_ratio()\n",
    "noise_ratio"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "myenv_python3.8_text_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
