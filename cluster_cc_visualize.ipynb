{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmu_nlp_hdd/suzhou03/data/quality/myenv_python3.10_quality/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "24/07/01 10:49:28 WARN util.NativeCodeLoader main: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/01 10:49:28 WARN shortcircuit.DomainSocketFactory main: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/07/01 10:49:28 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: Statistics from 2024-07-01 10:48:28 to 2024-07-01 10:49:28\n",
      "24/07/01 10:49:28 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: Tag                                                           Avg(ms)      Min      Max  Std Dev     95th     99th   99.5th   Count\n",
      "24/07/01 10:49:28 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: dataarch.hdfs.suzhou03.mmu_llm.12664-dtmachine.2.6.0U60.3.4-cdh5.10.0-CLIENT-RELEASE.hadoop-lt-cluster     0.00     0.00     0.00     0.00     0.00     0.00     0.00       1\n",
      "24/07/01 10:49:28 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: \n",
      "24/07/01 10:49:29 WARN hdfs.DFSClient main: hedgedFetchBlockByteRange waited 50ms to read from DatanodeInfoWithStorage[10.80.141.158:50010,DS-71250f8a-7c1f-40b4-85a4-3077cd67fe6d,DISK] LocatedBlock{BP-1561302996-10.46.134.41-1572878936413:blk_54524724578_53542907465; getBlockSize()=53731598; corrupt=false; offset=805306368; activeIndex=3; locs=[DatanodeInfoWithStorage[10.80.141.158:50010,DS-71250f8a-7c1f-40b4-85a4-3077cd67fe6d,DISK], DatanodeInfoWithStorage[10.80.141.156:50010,DS-7d52f5c1-4a21-4db7-a576-2be57ecf0a4f,DISK], DatanodeInfoWithStorage[10.80.131.223:50010,DS-39cd3165-6014-45d6-854a-ee43bdee3486,DISK]]} 53666062 65536; spawning hedged read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件包含 7 个行组\n",
      "文件模式: <pyarrow._parquet.ParquetSchema object at 0x7f2725c2d640>\n",
      "required group field_id=-1 spark_schema {\n",
      "  optional binary field_id=-1 text (String);\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]24/07/01 10:49:29 WARN hdfs.DFSClient main: hedgedFetchBlockByteRange waited 50ms to read from DatanodeInfoWithStorage[10.80.139.102:50010,DS-a87049f5-11b7-4f0e-8be0-d817ee9b1a19,DISK] LocatedBlock{BP-1561302996-10.46.134.41-1572878936413:blk_54524707756_53542890497; getBlockSize()=268435456; corrupt=false; offset=0; activeIndex=3; locs=[DatanodeInfoWithStorage[10.80.139.102:50010,DS-a87049f5-11b7-4f0e-8be0-d817ee9b1a19,DISK], DatanodeInfoWithStorage[10.80.122.216:50010,DS-122a497f-aa10-4676-8f3b-ccf3bc289174,DISK], DatanodeInfoWithStorage[10.80.139.99:50010,DS-99e35677-7266-4195-ace9-3f3c1c8ca3e7,DISK]]} 4 145656881; spawning hedged read\n",
      "  0%|          | 0/1 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完第 1 个行组\n",
      "已达到指定的最大块数 1，停止读取\n",
      "\n",
      "成功读取并解析数据\n",
      "总共解析的数据条数: 10000\n",
      "前5条解析后的内容:\n",
      "! 07/24/2000 mhamer /tmp/l-1-81-nc.onoff.bathnav! ...\n",
      "! 07/24/2000 mhamer /tmp/l-4-79-sc.onoff.bathgravn...\n",
      "! 07/24/2000 mhamer /tmp/l-7-77-wg.onoff.multichan...\n",
      "! Zum Reiherhorst 32, Stelle Jenny\n",
      "Ahoi,\n",
      "wollte no...\n",
      "! thread theory ♥\n",
      "« previous entry | next entry »\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# HDFS上Parquet文件的路径\n",
    "hdfs_path = \"viewfs://hadoop-lt-cluster/home/mmu_llm/dw/mmu_llm.db/customjtmath_2013_20/type=normal/part-04999-626445f5-ee23-4a80-b0bd-e35648f16988.c000.snappy.parquet\"\n",
    "\n",
    "def parse_json_content(json_str: str) -> str:\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        return json_data.get('content', '')\n",
    "    except json.JSONDecodeError:\n",
    "        return ''\n",
    "\n",
    "def read_and_parse_parquet(file_path: str, max_chunks: int = None, max_length: int = 50) -> Generator[Dict[str, str], None, None]:\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        print(f\"文件包含 {parquet_file.num_row_groups} 个行组\")\n",
    "        print(f\"文件模式: {parquet_file.schema}\")\n",
    "        \n",
    "        for i in tqdm(range(min(max_chunks or float('inf'), parquet_file.num_row_groups))):\n",
    "            table = parquet_file.read_row_group(i)\n",
    "            df = table.to_pandas()\n",
    "            if 'text' in df.columns:\n",
    "                for text in df['text']:\n",
    "                    content = parse_json_content(text)[:max_length]\n",
    "                    if content:\n",
    "                        yield content\n",
    "            print(f\"处理完第 {i+1} 个行组\")\n",
    "            if max_chunks and i + 1 >= max_chunks:\n",
    "                print(f\"已达到指定的最大块数 {max_chunks}，停止读取\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "\n",
    "# 读取并解析数据\n",
    "parsed_data = list(read_and_parse_parquet(hdfs_path, max_chunks=1))[:10000]\n",
    "\n",
    "if parsed_data:\n",
    "    print(f\"\\n成功读取并解析数据\")\n",
    "    print(f\"总共解析的数据条数: {len(parsed_data)}\")\n",
    "    print(\"前5条解析后的内容:\")\n",
    "    for item in parsed_data[:5]:\n",
    "        print(item[:100] + '...')  # 只打印每条内容的前100个字符\n",
    "\n",
    "    # 将解析后的数据转换为Hugging Face Dataset格式\n",
    "    # dataset = Dataset.from_list(parsed_data)\n",
    "    texts = parsed_data\n",
    "    gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "else:\n",
    "    print(\"无法读取或解析数据\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ClusterClassifier' from 'src' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/mmu_nlp_hdd/suzhou03/data/text-clustering/cluster_cc_visualize.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://kml-dtmachine-12664-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/data/text-clustering/cluster_cc_visualize.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://kml-dtmachine-12664-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/data/text-clustering/cluster_cc_visualize.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m ClusterClassifier\n\u001b[1;32m      <a href='vscode-notebook-cell://kml-dtmachine-12664-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/data/text-clustering/cluster_cc_visualize.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcycler\u001b[39;00m \u001b[39mimport\u001b[39;00m cycler\n\u001b[1;32m      <a href='vscode-notebook-cell://kml-dtmachine-12664-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/data/text-clustering/cluster_cc_visualize.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ClusterClassifier' from 'src' (unknown location)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 获取当前脚本的目录\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# 将 src 目录的父目录添加到 Python 路径\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from src.text_clustering import ClusterClassifier\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have a pandas DataFrame named pandas_df\n",
    "# Example:\n",
    "# pandas_df = pd.DataFrame({'content': [\"text1\", \"text2\", \"text3\"]})\n",
    "\n",
    "# Create an instance of ClusterClassifier\n",
    "cc = ClusterClassifier(embed_device=\"cpu\")  # Use \"cuda\" if you have a GPU\n",
    "\n",
    "# Run the pipeline on the 'content' column\n",
    "\n",
    "embs, labels, summaries = cc.fit(texts)\n",
    "\n",
    "# Customize color scheme (optional)\n",
    "default_cycler = (cycler(color=[\n",
    "    \"#0F0A0A\", \"#FF6600\", \"#FFBE00\", \"#496767\", \"#87A19E\",\n",
    "    \"#FF9200\", \"#0F3538\", \"#F8E08E\", \"#0F2021\", \"#FAFAF0\"\n",
    "]))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "\n",
    "# Visualize the results\n",
    "cc.show()\n",
    "\n",
    "# Save the classifier (optional)\n",
    "cc.save(\"./content_clusters\")\n",
    "\n",
    "# Print cluster summaries\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Cluster {i}: {summary}\")\n",
    "\n",
    "# If you want to classify new texts later:\n",
    "# new_texts = [\"Some new text\", \"Another new text\"]\n",
    "# cluster_labels, embeddings = cc.infer(new_texts, top_k=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3.10_text_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
