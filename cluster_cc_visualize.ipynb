{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmu_nlp_hdd/suzhou03/data/text-clustering/myenv_python3.8_text_clustering/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "24/06/25 11:39:30 WARN util.NativeCodeLoader main: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/25 11:39:30 WARN shortcircuit.DomainSocketFactory main: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/06/25 11:39:30 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: Statistics from 2024-06-25 11:38:30 to 2024-06-25 11:39:30\n",
      "24/06/25 11:39:30 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: Tag                                                           Avg(ms)      Min      Max  Std Dev     95th     99th   99.5th   Count\n",
      "24/06/25 11:39:30 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: dataarch.hdfs.suzhou03.mmu_llm.12755-dtmachine.2.6.0U60.3.4-cdh5.10.0-CLIENT-RELEASE.hadoop-lt-cluster     0.00     0.00     0.00     0.00     0.00     0.00     0.00       1\n",
      "24/06/25 11:39:30 INFO speed4j pool-simple-buffer-trigger-thread-[perf]: \n",
      "24/06/25 11:39:31 WARN hdfs.DFSClient main: hedgedFetchBlockByteRange waited 50ms to read from DatanodeInfoWithStorage[10.80.131.223:50010,DS-39cd3165-6014-45d6-854a-ee43bdee3486,DISK] LocatedBlock{BP-1561302996-10.46.134.41-1572878936413:blk_54524724578_53542907465; getBlockSize()=53731598; corrupt=false; offset=805306368; activeIndex=3; locs=[DatanodeInfoWithStorage[10.80.131.223:50010,DS-39cd3165-6014-45d6-854a-ee43bdee3486,DISK], DatanodeInfoWithStorage[10.80.141.156:50010,DS-7d52f5c1-4a21-4db7-a576-2be57ecf0a4f,DISK], DatanodeInfoWithStorage[10.80.141.158:50010,DS-71250f8a-7c1f-40b4-85a4-3077cd67fe6d,DISK]]} 53666062 65536; spawning hedged read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件包含 7 个行组\n",
      "文件模式: <pyarrow._parquet.ParquetSchema object at 0x7ff2c487f6c0>\n",
      "required group field_id=-1 spark_schema {\n",
      "  optional binary field_id=-1 text (String);\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]24/06/25 11:39:31 WARN hdfs.DFSClient main: hedgedFetchBlockByteRange waited 50ms to read from DatanodeInfoWithStorage[10.80.139.99:50010,DS-99e35677-7266-4195-ace9-3f3c1c8ca3e7,DISK] LocatedBlock{BP-1561302996-10.46.134.41-1572878936413:blk_54524707756_53542890497; getBlockSize()=268435456; corrupt=false; offset=0; activeIndex=3; locs=[DatanodeInfoWithStorage[10.80.139.99:50010,DS-99e35677-7266-4195-ace9-3f3c1c8ca3e7,DISK], DatanodeInfoWithStorage[10.80.122.216:50010,DS-122a497f-aa10-4676-8f3b-ccf3bc289174,DISK], DatanodeInfoWithStorage[10.80.139.102:50010,DS-a87049f5-11b7-4f0e-8be0-d817ee9b1a19,DISK]]} 4 145656881; spawning hedged read\n",
      "  0%|          | 0/1 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完第 1 个行组\n",
      "已达到指定的最大块数 1，停止读取\n",
      "\n",
      "成功读取并解析数据\n",
      "总共解析的数据条数: 10000\n",
      "前5条解析后的内容:\n",
      "! 07/24/2000 mhamer /tmp/l-1-81-nc.onoff.bathnav! ...\n",
      "! 07/24/2000 mhamer /tmp/l-4-79-sc.onoff.bathgravn...\n",
      "! 07/24/2000 mhamer /tmp/l-7-77-wg.onoff.multichan...\n",
      "! Zum Reiherhorst 32, Stelle Jenny\n",
      "Ahoi,\n",
      "wollte no...\n",
      "! thread theory ♥\n",
      "« previous entry | next entry »\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# HDFS上Parquet文件的路径\n",
    "hdfs_path = \"viewfs://hadoop-lt-cluster/home/mmu_llm/dw/mmu_llm.db/customjtmath_2013_20/type=normal/part-04999-626445f5-ee23-4a80-b0bd-e35648f16988.c000.snappy.parquet\"\n",
    "\n",
    "def parse_json_content(json_str: str) -> str:\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        return json_data.get('content', '')\n",
    "    except json.JSONDecodeError:\n",
    "        return ''\n",
    "\n",
    "def read_and_parse_parquet(file_path: str, max_chunks: int = None, max_length: int = 50) -> Generator[Dict[str, str], None, None]:\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        print(f\"文件包含 {parquet_file.num_row_groups} 个行组\")\n",
    "        print(f\"文件模式: {parquet_file.schema}\")\n",
    "        \n",
    "        for i in tqdm(range(min(max_chunks or float('inf'), parquet_file.num_row_groups))):\n",
    "            table = parquet_file.read_row_group(i)\n",
    "            df = table.to_pandas()\n",
    "            if 'text' in df.columns:\n",
    "                for text in df['text']:\n",
    "                    content = parse_json_content(text)[:max_length]\n",
    "                    if content:\n",
    "                        yield {'content': content}\n",
    "            print(f\"处理完第 {i+1} 个行组\")\n",
    "            if max_chunks and i + 1 >= max_chunks:\n",
    "                print(f\"已达到指定的最大块数 {max_chunks}，停止读取\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "\n",
    "# 读取并解析数据\n",
    "parsed_data = list(read_and_parse_parquet(hdfs_path, max_chunks=1))[:10000]\n",
    "\n",
    "if parsed_data:\n",
    "    print(f\"\\n成功读取并解析数据\")\n",
    "    print(f\"总共解析的数据条数: {len(parsed_data)}\")\n",
    "    print(\"前5条解析后的内容:\")\n",
    "    for item in parsed_data[:5]:\n",
    "        print(item['content'][:100] + '...')  # 只打印每条内容的前100个字符\n",
    "\n",
    "    # 将解析后的数据转换为Hugging Face Dataset格式\n",
    "    dataset = Dataset.from_list(parsed_data)\n",
    "\n",
    "    # 删除不再需要的变量以释放内存\n",
    "    del parsed_data\n",
    "    gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "else:\n",
    "    print(\"无法读取或解析数据\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmu_nlp_hdd/suzhou03/data/text-clustering/myenv_python3.8_text_clustering/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/mmu_nlp_hdd/suzhou03/data/text-clustering/myenv_python3.8_text_clustering/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 10.70it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 82.78it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 78.71it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 79.02it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 75.67it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 71.19it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 75.68it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 82.71it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 83.46it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:00<00:00, 82.69it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text clustering...\n",
      "Using DBSCAN (eps, nim_samples)=((0.08,), 50)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "from src.text_clustering import ClusterClassifier\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import config\n",
    "import gc\n",
    "\n",
    "# Configure HuggingFace datasets cache\n",
    "config.HF_DATASETS_CACHE = \"/mmu_nlp_hdd/suzhou03/data/model_zoo/hugging_face/datasets/cache\"\n",
    "\n",
    "# Create ./data directory (if it doesn't exist)\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Assume dataset is already loaded and contains a 'content' column\n",
    "texts = dataset[\"content\"]\n",
    "\n",
    "# Set custom color scheme\n",
    "colors = [\n",
    "    \"#0F0A0A\", \"#FF6600\", \"#FFBE00\", \"#496767\", \"#87A19E\",\n",
    "    \"#FF9200\", \"#0F3538\", \"#F8E08E\", \"#0F2021\", \"#FAFAF0\"\n",
    "]\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "\n",
    "# Create ClusterClassifier instance\n",
    "cc = ClusterClassifier(embed_device=\"cuda\")  # Use \"cuda\" if GPU is available, else use \"cpu\"\n",
    "\n",
    "# Function to embed texts in batches\n",
    "def batch_embed_texts(texts, batch_size=1000):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        embs = cc.embed(batch_texts)\n",
    "        all_embs.append(embs)\n",
    "        gc.collect()  # Force garbage collection to free memory\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# Embed texts in batches\n",
    "print(\"Embedding texts...\")\n",
    "embs = batch_embed_texts(texts)\n",
    "\n",
    "# Perform clustering on the combined embeddings\n",
    "print(\"Starting text clustering...\")\n",
    "clustering_result = cc.cluster(embs)\n",
    "\n",
    "# Print the clustering result to understand its structure\n",
    "print(f\"Clustering result: {clustering_result}\")\n",
    "\n",
    "# Assuming the clustering_result contains embeddings, labels, and summaries\n",
    "labels = clustering_result[0]\n",
    "summaries = clustering_result[1]\n",
    "\n",
    "# Display results\n",
    "print(\"Clustering complete, generating visualization...\")\n",
    "\n",
    "def custom_show(embs, labels):\n",
    "    # Use PCA to reduce embeddings to 2D if they're not already\n",
    "    if embs.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        embs_2d = pca.fit_transform(embs)\n",
    "    else:\n",
    "        embs_2d = embs\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi=300)\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax.scatter(embs_2d[mask, 0], embs_2d[mask, 1], c=[colors[i % len(colors)]],\n",
    "                   label=f'Cluster {label}', s=0.75, alpha=0.8)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_title(\"Text Clustering Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the custom show function\n",
    "custom_show(embs, labels)\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(data_dir, \"cc_parquet_data\")\n",
    "cc.save(save_path)\n",
    "print(f\"Clustering model saved to {save_path}\")\n",
    "\n",
    "# Print and save cluster summaries\n",
    "print(\"\\nCluster Summaries:\")\n",
    "summary_path = os.path.join(data_dir, \"cluster_summaries.txt\")\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    for i, summary in enumerate(summaries):\n",
    "        print(f\"Cluster {i}: {summary}\")\n",
    "        f.write(f\"Cluster {i}: {summary}\\n\")\n",
    "print(f\"Cluster summaries saved to {summary_path}\")\n",
    "\n",
    "# Example: Predict clusters for new texts\n",
    "new_texts = [\"This is a new math problem\", \"This is another text about history\"]\n",
    "cluster_labels, embeddings = cc.infer(new_texts, top_k=1)\n",
    "print(\"\\nCluster labels for new texts:\")\n",
    "for text, label in zip(new_texts, cluster_labels):\n",
    "    if isinstance(label, (list, np.ndarray)):\n",
    "        cluster = label[0]\n",
    "    else:\n",
    "        cluster = label\n",
    "    print(f\"Text: '{text}' -> Cluster: {cluster}\")\n",
    "\n",
    "# Save clustering results to CSV file\n",
    "results_df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'cluster': labels\n",
    "})\n",
    "results_path = os.path.join(data_dir, \"clustering_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nClustering results saved to {results_path}\")\n",
    "\n",
    "# Save visualization image\n",
    "plt.savefig(os.path.join(data_dir, \"cluster_visualization.png\"))\n",
    "print(f\"Cluster visualization saved to {os.path.join(data_dir, 'cluster_visualization.png')}\")\n",
    "\n",
    "print(\"\\nText clustering analysis complete. All output files have been saved to the ./data directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3.10_text_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
