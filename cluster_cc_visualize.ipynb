{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725006d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# HDFS上Parquet文件的路径\n",
    "hdfs_path = \"viewfs://hadoop-lt-cluster/home/mmu_llm/dw/mmu_llm.db/customjtmath_2013_20/type=normal/part-04999-626445f5-ee23-4a80-b0bd-e35648f16988.c000.snappy.parquet\"\n",
    "\n",
    "def parse_json_content(json_str: str) -> str:\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        return json_data.get('content', '')\n",
    "    except json.JSONDecodeError:\n",
    "        return ''\n",
    "\n",
    "def read_and_parse_parquet(file_path: str, max_chunks: int = None, max_length: int = 50) -> Generator[Dict[str, str], None, None]:\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        print(f\"文件包含 {parquet_file.num_row_groups} 个行组\")\n",
    "        print(f\"文件模式: {parquet_file.schema}\")\n",
    "        \n",
    "        for i in tqdm(range(min(max_chunks or float('inf'), parquet_file.num_row_groups))):\n",
    "            table = parquet_file.read_row_group(i)\n",
    "            df = table.to_pandas()\n",
    "            if 'text' in df.columns:\n",
    "                for text in df['text']:\n",
    "                    content = parse_json_content(text)[:max_length]\n",
    "                    if content:\n",
    "                        yield content\n",
    "            print(f\"处理完第 {i+1} 个行组\")\n",
    "            if max_chunks and i + 1 >= max_chunks:\n",
    "                print(f\"已达到指定的最大块数 {max_chunks}，停止读取\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "\n",
    "# 读取并解析数据\n",
    "parsed_data = list(read_and_parse_parquet(hdfs_path, max_chunks=1))[:1000]\n",
    "\n",
    "if parsed_data:\n",
    "    print(f\"\\n成功读取并解析数据\")\n",
    "    print(f\"总共解析的数据条数: {len(parsed_data)}\")\n",
    "    print(\"前5条解析后的内容:\")\n",
    "    for item in parsed_data[:5]:\n",
    "        print(item[:100] + '...')  # 只打印每条内容的前100个字符\n",
    "\n",
    "    # 将解析后的数据转换为Hugging Face Dataset格式\n",
    "    # dataset = Dataset.from_list(parsed_data)\n",
    "    texts = parsed_data\n",
    "    gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "else:\n",
    "    print(\"无法读取或解析数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111a0c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.text_clustering import ClusterClassifier\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have a pandas DataFrame named pandas_df\n",
    "# Example:\n",
    "# pandas_df = pd.DataFrame({'content': [\"text1\", \"text2\", \"text3\"]})\n",
    "\n",
    "# Create an instance of ClusterClassifier\n",
    "cc = ClusterClassifier(embed_device=\"cuda\")  # Use \"cuda\" if you have a GPU\n",
    "\n",
    "# Run the pipeline on the 'content' column\n",
    "\n",
    "embs, labels, summaries = cc.fit(texts)\n",
    "\n",
    "# Customize color scheme (optional)\n",
    "# default_cycler = (cycler(color=[\n",
    "#     \"#0F0A0A\", \"#FF6600\", \"#FFBE00\", \"#496767\", \"#87A19E\",\n",
    "#     \"#FF9200\", \"#0F3538\", \"#F8E08E\", \"#0F2021\", \"#FAFAF0\"\n",
    "# ]))\n",
    "# plt.rc('axes', prop_cycle=default_cycler)\n",
    "\n",
    "# Visualize the results\n",
    "cc.show(interactive=False)\n",
    "\n",
    "# Save the classifier (optional)\n",
    "cc.save(\"./content_clusters\")\n",
    "\n",
    "# Print cluster summaries\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Cluster {i}: {summary}\")\n",
    "\n",
    "# If you want to classify new texts later:\n",
    "# new_texts = [\"Some new text\", \"Another new text\"]\n",
    "# cluster_labels, embeddings = cc.infer(new_texts, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_cluster_proportions(labels, summaries):\n",
    "    # Count the number of items in each cluster\n",
    "    cluster_counts = Counter(labels)\n",
    "    \n",
    "    # Sort the clusters by their labels, excluding -1 (which is typically used for noise)\n",
    "    sorted_clusters = sorted([item for item in cluster_counts.items() if item[0] != -1])\n",
    "    \n",
    "    # Separate the labels and counts\n",
    "    cluster_labels, counts = zip(*sorted_clusters)\n",
    "    \n",
    "    # Calculate the proportion of noise points (label -1)\n",
    "    noise_count = cluster_counts.get(-1, 0)\n",
    "    total_count = sum(counts) + noise_count\n",
    "    noise_proportion = noise_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    # Create a color palette\n",
    "    colors = sns.color_palette(\"husl\", len(cluster_labels))\n",
    "    \n",
    "    # Create cluster labels with summaries\n",
    "    cluster_summaries = [f'Cluster {label}: {summaries.get(label, \"No summary\")}' for label in cluster_labels]\n",
    "    \n",
    "    # Create a pie chart\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    wedges, texts, autotexts = plt.pie(counts, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "    \n",
    "    # Ensure the percentage text is visible\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('black')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(10)\n",
    "    \n",
    "    plt.title('Proportion of Data Points in Each Cluster', fontsize=16)\n",
    "    \n",
    "    # Add a note about noise proportion\n",
    "    plt.annotate(f'Noise: {noise_proportion:.1%}', xy=(0.95, 0.05), xycoords='axes fraction', \n",
    "                 horizontalalignment='right', verticalalignment='bottom', fontsize=10)\n",
    "    \n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    \n",
    "    # Add a legend with cluster summaries\n",
    "    plt.legend(wedges, cluster_summaries, \n",
    "               title=\"Clusters and Summaries\", loc=\"center left\", \n",
    "               bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have already run the clustering and have the labels and summaries\n",
    "# labels = cc.cluster_labels\n",
    "# summaries = cc.cluster_summaries\n",
    "\n",
    "visualize_cluster_proportions(labels, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be978d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccb16d",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "noise_ratio = cc.calculate_noise_ratio()\n",
    "noise_ratio"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
